{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zTMqss9QlpKl"
   },
   "source": [
    "# Capstone Project: Slogan Classifier and Generator\n",
    "\n",
    "In this capstone project you will train a Long Short-Term Memory (LSTM) model to generate slogans for businesses based on their industry, and also train a classifier to predict the industry based on a given slogan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R11lkWvYvXC-"
   },
   "source": [
    "## Libraries\n",
    "We recommend running this notebook using [Google Colab](https://colab.google/) however if you choose to use your local machine you will need to install spaCy before starting.\n",
    "\n",
    "To install spaCy, refer to the installation instructions provided on the spaCy [website](https://spacy.io/usage). Note you may need to install an older version of Python that is compatible with spaCy. You can create a virtual environment for this project to install the specific version of Python that you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "XU54-5mpnujt"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-11 00:51:56.195127: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/admin/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Core Imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import spacy \n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aedbu0lBovia"
   },
   "source": [
    "## Loading and viewing the dataset\n",
    "\n",
    "- Load the slogan dataset into a variable called data.\n",
    "- Extract relevant columns in a variable called df.\n",
    "- Handle missing values.\n",
    "\n",
    "Do **not** change the column names.\n",
    "\n",
    "If you are using Google Colab you will need mount your Google Drive as follows:  \n",
    "`from google.colab import drive`  \n",
    "`drive.mount('/content/drive')`  \n",
    "\n",
    "The path you use when loading your data will look something like this if you are using your Google Drive:  \n",
    "\"/content/drive/MyDrive/Colab Notebooks/slogan-valid.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "OH39XzGKpTpX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Loaded CSV from: slogan-valid.csv\n",
      "[info] DataFrame shape: (5346, 12)\n",
      "[info] Columns: ['desc', 'output', 'type', 'company', 'industry', 'url', 'alias', 'desc_masked', 'output_masked', 'ent_dict', 'unsupported', 'first_pos']\n"
     ]
    }
   ],
   "source": [
    "# Load data from local machine\n",
    "# Goal: create a pandas DataFrame named df.\n",
    "# read from a local path or a small set of fallbacks.\n",
    "\n",
    "# Local filesystem (default) \n",
    "#  If you know your file path, set it here:\n",
    "DATA_PATH = os.environ.get(\"DATA_PATH\", \"\").strip()  # you can export DATA_PATH or leave blank\n",
    "\n",
    "# If DATA_PATH is empty, I try a few sensible candidates.\n",
    "CANDIDATES = [\n",
    "    DATA_PATH,\n",
    "    \"/mnt/data/slogan-valid.csv\",     # common path if working in a hosted VM / uploaded file\n",
    "    \"./slogan-valid.csv\",             # current directory\n",
    "    \"../slogan-valid.csv\",            # parent directory\n",
    "]\n",
    "\n",
    "def _read_csv_safely(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Try reading a CSV with utf-8 first, then fall back to latin-1.\n",
    "    Returns a DataFrame on success or raises the last exception.\n",
    "    \"\"\"\n",
    "    last_err = None\n",
    "    for enc in (\"utf-8\", \"latin-1\"):\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc)\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "    # If attempts failed, raise the last encountered error\n",
    "    raise last_err\n",
    "\n",
    "df = None\n",
    "for c in CANDIDATES:\n",
    "    p = Path(c) if c else None\n",
    "    if p and p.is_file():\n",
    "        df = _read_csv_safely(p)\n",
    "        print(f\"[info] Loaded CSV from: {p}\")\n",
    "        break\n",
    "\n",
    "# Final checks \n",
    "if df is None:\n",
    "    raise FileNotFoundError(\n",
    "        \"CSV not found. Set DATA_PATH to your file or place 'slogan-valid.csv' in the working folder. \"\n",
    "        \"If using Colab, mount Drive and point to the Drive path.\"\n",
    "    )\n",
    "\n",
    "print(f\"[info] DataFrame shape: {df.shape}\")\n",
    "print(f\"[info] Columns: {list(df.columns)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wj6Whax7nSxQ"
   },
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Since we are working with textual data, we need software that understands natural language. For this, we'll use a library for processing text called **spaCy**. Using spaCy, we'll break the text into smaller units called tokens that are easier for the machine to process. This process is called **tokenisation**. We'll also convert all text to lowercase and remove punctuation because this information is not necessary for our models. Run the code below, and your dataframe (df) will gain a new column called **'processed_slogan'** which contains the preprocessed text.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ZcIWV7IXp9rt"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>desc</th>\n",
       "      <th>output</th>\n",
       "      <th>type</th>\n",
       "      <th>company</th>\n",
       "      <th>industry</th>\n",
       "      <th>url</th>\n",
       "      <th>alias</th>\n",
       "      <th>desc_masked</th>\n",
       "      <th>output_masked</th>\n",
       "      <th>ent_dict</th>\n",
       "      <th>unsupported</th>\n",
       "      <th>first_pos</th>\n",
       "      <th>processed_slogan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The latest &lt;company&gt; &amp; Point of Sale tech for ...</td>\n",
       "      <td>Taking Care of Small Business Technology</td>\n",
       "      <td>headline_long</td>\n",
       "      <td>eftpos warehouse</td>\n",
       "      <td>computer hardware</td>\n",
       "      <td>eftposwarehouse.co.nz</td>\n",
       "      <td>Eftpos Warehouse</td>\n",
       "      <td>The latest &lt;company&gt; &amp; Point of Sale tech for ...</td>\n",
       "      <td>Taking Care of Small Business Technology</td>\n",
       "      <td>{'[date]': 'monthly'}</td>\n",
       "      <td>False</td>\n",
       "      <td>VB</td>\n",
       "      <td>taking care of small business technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Easily deliver personalized activities that en...</td>\n",
       "      <td>Build World-Class Recreation Programs</td>\n",
       "      <td>headline</td>\n",
       "      <td>welbi</td>\n",
       "      <td>health, wellness and fitness</td>\n",
       "      <td>welbi.co</td>\n",
       "      <td>Welbi</td>\n",
       "      <td>Easily deliver personalized activities that en...</td>\n",
       "      <td>Build World-Class Recreation Programs</td>\n",
       "      <td>{}</td>\n",
       "      <td>False</td>\n",
       "      <td>VB</td>\n",
       "      <td>build world class recreation programs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Powerful lead generation software that convert...</td>\n",
       "      <td>Most Powerful Lead Generation Software for Mar...</td>\n",
       "      <td>headline_long</td>\n",
       "      <td>optinmonster</td>\n",
       "      <td>internet</td>\n",
       "      <td>optinmonster.com</td>\n",
       "      <td>Optinmonster</td>\n",
       "      <td>Powerful lead generation software that convert...</td>\n",
       "      <td>Most Powerful Lead Generation Software for Mar...</td>\n",
       "      <td>{}</td>\n",
       "      <td>False</td>\n",
       "      <td>JJ</td>\n",
       "      <td>most powerful lead generation software for mar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Twine matches companies to the best digital an...</td>\n",
       "      <td>Hire quality freelancers for your job</td>\n",
       "      <td>headline_long</td>\n",
       "      <td>twine.fm</td>\n",
       "      <td>internet</td>\n",
       "      <td>twine.fm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twine matches companies to the best digital an...</td>\n",
       "      <td>Hire quality freelancers for your job</td>\n",
       "      <td>{'[number]': 'over 260,000'}</td>\n",
       "      <td>False</td>\n",
       "      <td>VB</td>\n",
       "      <td>hire quality freelancers for your job</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Financial Advisers Norwich, Norfolk - &lt;company...</td>\n",
       "      <td>Financial Advisers Norwich, Norfolk</td>\n",
       "      <td>headline</td>\n",
       "      <td>mcb financial services ltd</td>\n",
       "      <td>financial services</td>\n",
       "      <td>mcbfinancialservices.co.uk</td>\n",
       "      <td>Mcb Financial Services</td>\n",
       "      <td>Financial Advisers [country], [country1] - &lt;co...</td>\n",
       "      <td>Financial Advisers [country], [country1]</td>\n",
       "      <td>{'[country]': 'Norwich', '[country1]': 'Norfolk'}</td>\n",
       "      <td>False</td>\n",
       "      <td>NN</td>\n",
       "      <td>financial advisers norwich norfolk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                desc  \\\n",
       "0  The latest <company> & Point of Sale tech for ...   \n",
       "1  Easily deliver personalized activities that en...   \n",
       "2  Powerful lead generation software that convert...   \n",
       "3  Twine matches companies to the best digital an...   \n",
       "4  Financial Advisers Norwich, Norfolk - <company...   \n",
       "\n",
       "                                              output           type  \\\n",
       "0           Taking Care of Small Business Technology  headline_long   \n",
       "1              Build World-Class Recreation Programs       headline   \n",
       "2  Most Powerful Lead Generation Software for Mar...  headline_long   \n",
       "3              Hire quality freelancers for your job  headline_long   \n",
       "4                Financial Advisers Norwich, Norfolk       headline   \n",
       "\n",
       "                      company                      industry  \\\n",
       "0            eftpos warehouse             computer hardware   \n",
       "1                       welbi  health, wellness and fitness   \n",
       "2                optinmonster                      internet   \n",
       "3                    twine.fm                      internet   \n",
       "4  mcb financial services ltd            financial services   \n",
       "\n",
       "                          url                   alias  \\\n",
       "0       eftposwarehouse.co.nz        Eftpos Warehouse   \n",
       "1                    welbi.co                   Welbi   \n",
       "2            optinmonster.com            Optinmonster   \n",
       "3                    twine.fm                     NaN   \n",
       "4  mcbfinancialservices.co.uk  Mcb Financial Services   \n",
       "\n",
       "                                         desc_masked  \\\n",
       "0  The latest <company> & Point of Sale tech for ...   \n",
       "1  Easily deliver personalized activities that en...   \n",
       "2  Powerful lead generation software that convert...   \n",
       "3  Twine matches companies to the best digital an...   \n",
       "4  Financial Advisers [country], [country1] - <co...   \n",
       "\n",
       "                                       output_masked  \\\n",
       "0           Taking Care of Small Business Technology   \n",
       "1              Build World-Class Recreation Programs   \n",
       "2  Most Powerful Lead Generation Software for Mar...   \n",
       "3              Hire quality freelancers for your job   \n",
       "4           Financial Advisers [country], [country1]   \n",
       "\n",
       "                                            ent_dict  unsupported first_pos  \\\n",
       "0                              {'[date]': 'monthly'}        False        VB   \n",
       "1                                                 {}        False        VB   \n",
       "2                                                 {}        False        JJ   \n",
       "3                       {'[number]': 'over 260,000'}        False        VB   \n",
       "4  {'[country]': 'Norwich', '[country1]': 'Norfolk'}        False        NN   \n",
       "\n",
       "                                    processed_slogan  \n",
       "0           taking care of small business technology  \n",
       "1              build world class recreation programs  \n",
       "2  most powerful lead generation software for mar...  \n",
       "3              hire quality freelancers for your job  \n",
       "4                 financial advisers norwich norfolk  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load spaCy model for text processing\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text_lower = text.lower()\n",
    "    doc = nlp(text_lower)\n",
    "\n",
    "    processed_tokens = []\n",
    "\n",
    "    for token in doc:\n",
    "        if not token.is_punct:\n",
    "            processed_tokens.append(token.text)\n",
    "\n",
    "    return \" \".join(processed_tokens)\n",
    "\n",
    "df[\"processed_slogan\"] = df[\"output\"].apply(preprocess_text)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tXpVm-kItirs"
   },
   "source": [
    "We want our model to generate **industry-specific** slogans. If we use the 'processed_slogan' column as it is, we'll be leaving out crucial context - the industries of the companies behind those slogans. To fix this, we'll create a new **'modified_slogan'** column that adds the industry name to the front of processed slogan.  \n",
    "\n",
    "For example:  \n",
    "\n",
    "> industry = 'computer hardware'  \n",
    "processed_slogan = 'taking care of small business technology'  \n",
    "modified_slogan = 'computer hardware taking care of small business technology'\n",
    "\n",
    "Write code in the cell below to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "VLNM3yK7ti6x"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>industry</th>\n",
       "      <th>processed_slogan</th>\n",
       "      <th>modified_slogan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>computer hardware</td>\n",
       "      <td>taking care of small business technology</td>\n",
       "      <td>computer hardware taking care of small busines...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>health, wellness and fitness</td>\n",
       "      <td>build world class recreation programs</td>\n",
       "      <td>health, wellness and fitness build world class...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>internet</td>\n",
       "      <td>most powerful lead generation software for mar...</td>\n",
       "      <td>internet most powerful lead generation softwar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>internet</td>\n",
       "      <td>hire quality freelancers for your job</td>\n",
       "      <td>internet hire quality freelancers for your job</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>financial services</td>\n",
       "      <td>financial advisers norwich norfolk</td>\n",
       "      <td>financial services financial advisers norwich ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       industry  \\\n",
       "0             computer hardware   \n",
       "1  health, wellness and fitness   \n",
       "2                      internet   \n",
       "3                      internet   \n",
       "4            financial services   \n",
       "\n",
       "                                    processed_slogan  \\\n",
       "0           taking care of small business technology   \n",
       "1              build world class recreation programs   \n",
       "2  most powerful lead generation software for mar...   \n",
       "3              hire quality freelancers for your job   \n",
       "4                 financial advisers norwich norfolk   \n",
       "\n",
       "                                     modified_slogan  \n",
       "0  computer hardware taking care of small busines...  \n",
       "1  health, wellness and fitness build world class...  \n",
       "2  internet most powerful lead generation softwar...  \n",
       "3     internet hire quality freelancers for your job  \n",
       "4  financial services financial advisers norwich ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create 'modified_slogan' by prefixing the industry to the processed slogan\n",
    "\n",
    "# Identify the industry column name - common variants.\n",
    "IND_COL = None\n",
    "for cand in [\"industry\", \"Industry\", \"sector\", \"category\"]:\n",
    "    if cand in df.columns:\n",
    "        IND_COL = cand\n",
    "        break\n",
    "if IND_COL is None:\n",
    "    raise KeyError(\"Could not find an industry column. Expected one of: 'industry', 'Industry', 'sector', 'category'.\")\n",
    "\n",
    "# Sanity check: ensure 'processed_slogan' exists.\n",
    "if \"processed_slogan\" not in df.columns:\n",
    "    raise KeyError(\"Expected column 'processed_slogan' not found. Run the preprocessing cell first.\")\n",
    "\n",
    "# Build a helper that joins 'industry' + 'processed_slogan' cleanly.\n",
    "#    - Lowercase industry for consistency with processed text\n",
    "#    - Handle missing values\n",
    "#    - Collapse extra spaces\n",
    "def make_modified_slogan(industry_val, processed):\n",
    "    ind = str(industry_val).strip().lower() if pd.notna(industry_val) else \"\"\n",
    "    txt = str(processed).strip() if pd.notna(processed) else \"\"\n",
    "    # If industry is missing, just return the processed text\n",
    "    if not ind:\n",
    "        return txt\n",
    "    # If processed text is missing, return just the industry\n",
    "    if not txt:\n",
    "        return ind\n",
    "    # Otherwise, prefix \"industry\" + space + processed text\n",
    "    return f\"{ind} {txt}\"\n",
    "\n",
    "# Create the new column.\n",
    "df[\"modified_slogan\"] = df[[IND_COL, \"processed_slogan\"]].apply(\n",
    "    lambda row: make_modified_slogan(row[IND_COL], row[\"processed_slogan\"]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "#  Quick preview to verify the transformation.\n",
    "df[[IND_COL, \"processed_slogan\", \"modified_slogan\"]].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_AJyWXVnwyNQ"
   },
   "source": [
    "Now we need to get data to train our model. We have textual data which we will need to represent numerically for our model to learn from it.  \n",
    "The code below does the following:\n",
    "1. Tokenizes a dataset of slogans.\n",
    "2. Converts words to numerical indices.\n",
    "3. Creates input sequences using the numerical indices.  \n",
    "\n",
    "Here's how it works. From the 'modified_slogan' column, we take the slogan \"computer hardware taking care of small business technology\". The tokenisation process will convert words into their corresponding indices:  \n",
    "\n",
    "<center>\n",
    "\n",
    "| Word         | Token Index |\n",
    "|-------------|-------|\n",
    "| \"computer\"  | 1     |\n",
    "| \"hardware\"  | 2     |\n",
    "| \"taking\"    | 3     |\n",
    "| \"care\"      | 4     |\n",
    "| \"of\"        | 5     |\n",
    "| \"small\"     | 6     |\n",
    "| \"business\"  | 7     |\n",
    "| \"technology\"| 8     |\n",
    "\n",
    "</center>\n",
    "\n",
    "So the tokenized list is:\n",
    "\n",
    "<center>\n",
    "[1, 2, 3, 4, 5, 6, 7, 8]\n",
    "</center>\n",
    "\n",
    "When creating input sequences for training, the loop generates progressively longer sequences.\n",
    "\n",
    "<center>\n",
    "\n",
    "| Token Index Sequence               | Corresponding Slogan                                 |\n",
    "|------------------------------|-----------------------------------------------------|\n",
    "| [1, 2]                       | \"computer hardware\"                                |\n",
    "| [1, 2, 3]                    | \"computer hardware taking\"                        |\n",
    "| [1, 2, 3, 4]                 | \"computer hardware taking care\"                   |\n",
    "| [1, 2, 3, 4, 5]              | \"computer hardware taking care of\"                |\n",
    "| [1, 2, 3, 4, 5, 6]           | \"computer hardware taking care of small\"          |\n",
    "| [1, 2, 3, 4, 5, 6, 7]        | \"computer hardware taking care of small business\" |\n",
    "| [1, 2, 3, 4, 5, 6, 7, 8]     | \"computer hardware taking care of small business technology\" |\n",
    "\n",
    "</center>\n",
    "\n",
    "Instead of training the model on only **complete slogans**, we provide partial phrases which will help the model learn how words connect over time. This will make it better at predicting the next word when generating slogans.  \n",
    "\n",
    "Run the cell block below to generate the input sequences. Be sure to read the comments to understand what the code is doing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "1Sl1Id28oXgu"
   },
   "outputs": [],
   "source": [
    "'''** Clean up comments'''\n",
    "\n",
    "# Tokenizer to convert words into numerical values tokens\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Tokenizer learns words in dataset\n",
    "tokenizer.fit_on_texts(df[\"modified_slogan\"])\n",
    "\n",
    "# Total number of unique words in learned vocabulary\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Dictionary mapping words to its numerical index: index based on frequency i.e., more freq => lower index\n",
    "tokenizer.word_index\n",
    "\n",
    "# Creating input sequences\n",
    "# Initialise list to store the input sequences\n",
    "input_sequences = []\n",
    "\n",
    "# Iterate over processed slogans\n",
    "for line in df[\"modified_slogan\"]:\n",
    "\n",
    "    # Convert slogans to token sequences\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0] # returns list containing list of words indices; extracting inner list [0]\n",
    "\n",
    "    # token_list is a list of tokenized word INDICES\n",
    "    # Building list of progressively longer input sequences for better training\n",
    "    for i in range(1, len(token_list)):\n",
    "        input_sequences.append(token_list[:i+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8lY8QgnST-MA"
   },
   "source": [
    "The input sequences created above are of **varying lengths**, which will be a problem when training our LSTM model. LSTMs require input sequences of **equal length**. So, we need to **pad** shorter sequences by **prepending zeros** until they match the length of the longest sequence.  \n",
    "\n",
    "For example, if the longest sequence has **10 tokens**, our padded sequences will look like this:\n",
    "\n",
    "<center>\n",
    "\n",
    "| Input Sequence                     | Padded Sequence                         |\n",
    "|-------------------------------------|-----------------------------------------|\n",
    "| [1, 2]                              | [0, 0, 0, 0, 0, 0, 0, 0, 1, 2]         |\n",
    "| [1, 2, 3]                           | [0, 0, 0, 0, 0, 0, 0, 1, 2, 3]         |\n",
    "| [1, 2, 3, 4]                        | [0, 0, 0, 0, 0, 0, 1, 2, 3, 4]         |\n",
    "| [1, 2, 3, 4, 5]                     | [0, 0, 0, 0, 0, 1, 2, 3, 4, 5]         |\n",
    "| [1, 2, 3, 4, 5, 6]                  | [0, 0, 0, 0, 1, 2, 3, 4, 5, 6]         |\n",
    "| [1, 2, 3, 4, 5, 6, 7]               | [0, 0, 0, 1, 2, 3, 4, 5, 6, 7]         |\n",
    "| [1, 2, 3, 4, 5, 6, 7, 8]            | [0, 0, 1, 2, 3, 4, 5, 6, 7, 8]         |\n",
    "\n",
    "</center>\n",
    "\n",
    "In the cell below, write code that **finds the length of the longest sequence** in **input_sequences** and stores this value in a variable named **max_seq_len**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-xqxBtBoT-vF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Number of sequences: 34,736\n",
      "[info] Longest sequence length (max_seq_len): 15\n"
     ]
    }
   ],
   "source": [
    "# Find the length of the longest token sequence\n",
    "# input_sequences is a list of lists - each inner list is a sequence of token IDs\n",
    "\n",
    "if not input_sequences:\n",
    "    raise ValueError(\"input_sequences is empty. Make sure the tokenisation cell ran and produced sequences.\")\n",
    "\n",
    "# Compute the maximum length across all sequences\n",
    "max_seq_len = max(len(seq) for seq in input_sequences)\n",
    "\n",
    "print(f\"[info] Number of sequences: {len(input_sequences):,}\")\n",
    "print(f\"[info] Longest sequence length (max_seq_len): {max_seq_len}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RGE2b-LsYPBh"
   },
   "source": [
    "Run the cell below to pad the input sequences so they are all the same length as **max_seq_length**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "3sRJZGE-YPo8"
   },
   "outputs": [],
   "source": [
    "input_sequences = pad_sequences(input_sequences, maxlen=max_seq_len, padding=\"pre\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xv7pgFrdb6_G"
   },
   "source": [
    "## Training Data for Slogan Generator\n",
    "\n",
    "The input sequences generated will be used as our training data. Our LSTM needs to learn how to predict the **next word** in a sequence.  \n",
    "\n",
    "The inputs for our model will be the input sequences **excluding the last token index** and the outputs will be the **last token index**.  \n",
    "\n",
    "As an example, let us use the input sequence [0, 0, 1, 2, 3, 4, 5, 6, 7, 8] and say it corresponds to the slogan \"computer hardware taking care of small business technology\". When training the model:\n",
    "\n",
    "> Our input **x** will be the input sequence [0, 0, 1, 2, 3, 4, 5, 6, 7] corresponding to \"computer hardware taking care of small\".  \n",
    "> Our output **y** will be [8] which corresponds to \"business\".  \n",
    "\n",
    "In the code cell below, use `input_sequences` to create the following two variables:\n",
    "1. **X_gen** which contains the input sequences excluding the last token index.\n",
    "2. **y_gen** which contains the last token index of the input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ARUDFwz9ilkF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Training samples: 34,736\n",
      "[info] X_gen shape: (34736, 14)  (timesteps = 14)\n",
      "[info] y_gen shape: (34736,)  (int token IDs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          11],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11,\n",
       "         236]], dtype=int32),\n",
       " array([ 236, 2708], dtype=int32))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build training pairs for the slogan GENERATOR\n",
    "# For each token sequence [w1, w2, ..., wN]:\n",
    "#   X_gen gets the prefix [w1, w2, ..., w(N-1)]\n",
    "#   y_gen gets the next-token target wN\n",
    "\n",
    "#  Ensure input_sequences is a Python list of lists\n",
    "if isinstance(input_sequences, np.ndarray):\n",
    "    input_sequences = input_sequences.tolist()\n",
    "\n",
    "if input_sequences is None or len(input_sequences) == 0:\n",
    "    raise ValueError(\"input_sequences is empty. Re-run the tokenisation step that builds input_sequences.\")\n",
    "\n",
    "# Split into prefixes (inputs) and next-token targets (outputs)\n",
    "X_gen_raw, y_gen = [], []\n",
    "for seq in input_sequences:\n",
    "    # need at least length 2 to form (prefix, target)\n",
    "    if len(seq) < 2:\n",
    "        continue\n",
    "    # all but the last token   \n",
    "    X_gen_raw.append(seq[:-1])  \n",
    "    # the last token is the label\n",
    "    y_gen.append(seq[-1])          \n",
    "\n",
    "# Convert targets to a compact int array\n",
    "y_gen = np.array(y_gen, dtype=np.int32)\n",
    "\n",
    "# Figure out padding length for prefixes\n",
    "# If you computed max_seq_len earlier on FULL sequences, prefixes are length (max_seq_len - 1)\n",
    "if \"max_seq_len\" not in globals():\n",
    "    max_seq_len = max(len(s) for s in input_sequences)\n",
    "\n",
    "if max_seq_len <= 1:\n",
    "    raise ValueError(\"max_seq_len is <= 1. Check your tokenisation; sequences are too short to train a next-word model.\")\n",
    "\n",
    "# Pad prefixes to equal length for LSTM input\n",
    "X_gen = pad_sequences(\n",
    "    X_gen_raw,\n",
    "    maxlen=max_seq_len - 1,   # pad to longest prefix length\n",
    "    padding=\"pre\",            # prepend zeros (as per brief)\n",
    "    truncating=\"pre\"\n",
    ")\n",
    "\n",
    "print(f\"[info] Training samples: {X_gen.shape[0]:,}\")\n",
    "print(f\"[info] X_gen shape: {X_gen.shape}  (timesteps = {X_gen.shape[1]})\")\n",
    "print(f\"[info] y_gen shape: {y_gen.shape}  (int token IDs)\")\n",
    "# Quick peek\n",
    "X_gen[:2], y_gen[:2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XXKjM3KPkUHu"
   },
   "source": [
    "The model will output the next word of a sequence over a probability distribution. We need to encode our output variable for this to be possible.\n",
    "\n",
    "In the code cell below, write code that will apply one-hot encoding to **y_gen** using `tf.keras.utils.to_categorical()`. **Maintain the same variable name**.  \n",
    "\n",
    "*Hint: set the `num_classes` (number of classes) parameter to the total number of unique words in the learned vocabulary. You can access this value through a variable that was created when generating input sequences earlier.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "0FpxfR4rkUTK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] One-hot targets shape: (34736, 6046)\n",
      "[info] Vocabulary size (total_words): 6046\n",
      "[info] Example: first target index = 236\n"
     ]
    }
   ],
   "source": [
    "# One-hot encode the generator targets (y_gen) so the model can predict a word distribution \n",
    "\n",
    "# y_gen currently holds integer token IDs \n",
    "# I convert these integers into one-hot rows of length = vocabulary size.\n",
    "# 'total_words' was defined when I built the tokenizer: len(tokenizer.word_index) + 1\n",
    "if \"total_words\" not in globals():\n",
    "    total_words = len(tokenizer.word_index) + 1  # safety fallback\n",
    "\n",
    "# Apply one-hot encoding in-place \n",
    "y_gen = to_categorical(y_gen, num_classes=total_words, dtype=\"float32\")\n",
    "\n",
    "# (num_samples, total_words)\n",
    "print(f\"[info] One-hot targets shape: {y_gen.shape}\")   \n",
    "print(f\"[info] Vocabulary size (total_words): {total_words}\")\n",
    "# Peek at the first row's non-zero index \n",
    "nz = y_gen[0].argmax()\n",
    "print(f\"[info] Example: first target index = {nz}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TUraZ3Rbn7Az"
   },
   "source": [
    "## Slogan Generator Architecture\n",
    "\n",
    "In the code cell that follows, configure the LSTM following these steps:\n",
    "\n",
    "1. Create a sequential model using `tf.keras.models.Sequential()`. This model will have an embedding layer, two LSTM layers, and a dense output layer.\n",
    "2. Add an embedding layer that converts words into dense vector representations. This layer should:\n",
    "> *   Have `total_words`as the vocabulary size.\n",
    "> *   Use 100 as an embedding dimension.\n",
    "> *   Takes an input length of `max_seq_len - 1` (excludes the target word).\n",
    "3. Add two LSTM layers.\n",
    "> *   The first LSTM layer should have 150 **and** set `return_sequences` to `True`.\n",
    "> *   The second LSTM layer should have 100 units.\n",
    "4. Add a dense output layer which:\n",
    "> *   Uses `total_words` as the number of units (one for each word in the vocabulary).\n",
    "> *   Uses a softmax activation function.\n",
    "5. Use `Sequential` to put everything together in the correct order to complete the architecture of the LSTM model called **gen_model**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "0wnqMaqZn7QP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 14, 100)           604600    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 14, 150)           150600    \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 100)               100400    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 6046)              610646    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1466246 (5.59 MB)\n",
      "Trainable params: 1466246 (5.59 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Slogan Generator: LSTM architecture (Sequential)\n",
    "# Spec:\n",
    "#    Embedding-vocab_size=total_words, embed_dim=100, input_length=max_seq_len-1\n",
    "#    LSTM(150, return_sequences=True)\n",
    "#    LSTM(100)\n",
    "#    Dense(total_words, activation=\"softmax\")\n",
    "\n",
    "# Basic sanity checks (helps catch earlier-cell mistakes)\n",
    "assert \"total_words\" in globals() and isinstance(total_words, int) and total_words > 0, \\\n",
    "    \"total_words is missing or invalid. Re-run the tokeniser cell.\"\n",
    "assert \"max_seq_len\" in globals() and isinstance(max_seq_len, int) and max_seq_len > 1, \\\n",
    "    \"max_seq_len is missing or invalid. Compute it from input_sequences first.\"\n",
    "\n",
    "gen_model = Sequential([\n",
    "    # Turn token IDs into dense vectors that the LSTM can learn over\n",
    "    Embedding(\n",
    "        # vocabulary size\n",
    "        input_dim=total_words, \n",
    "        # embedding dimension (per instructions)\n",
    "        output_dim=100,          \n",
    "        # prefix length (target word excluded)\n",
    "        input_length=max_seq_len - 1  \n",
    "    ),\n",
    "    # First recurrent layer (keeps sequence for the next LSTM)\n",
    "    LSTM(150, return_sequences=True),\n",
    "    # Second recurrent layer (final sequence encoding)\n",
    "    LSTM(100),\n",
    "    # Predict a distribution over the next word (one logit per vocab item)\n",
    "    Dense(total_words, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# Preview the parameter counts and shapes \n",
    "gen_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7bBvS8m0rvjW"
   },
   "source": [
    "In the code cell below, compile `gen_model` using `categorical_crossentropy` loss, an Adam optimiser, and an appropriate metric of your choice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Tb6NRsGir6XN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] gen_model compiled: loss=categorical_crossentropy, optimizer=Adam(1e-3), metrics=['accuracy']\n"
     ]
    }
   ],
   "source": [
    "#Compile the generator model \n",
    "# I predict a single next-token class over the vocabulary → categorical cross-entropy.\n",
    "# Adam is a good default optimiser for sequence models. I’ll track accuracy for a quick signal.\n",
    "\n",
    "gen_model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=Adam(learning_rate=1e-3),\n",
    "    # simple, readable metric for next-word prediction\n",
    "    metrics=[\"accuracy\"]          \n",
    ")\n",
    "\n",
    "print(\"[info] gen_model compiled: loss=categorical_crossentropy, optimizer=Adam(1e-3), metrics=['accuracy']\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZwdAZH-ysr9d"
   },
   "source": [
    "## Slogan Generation\n",
    "\n",
    "In the code cell below, fit the compiled model on the inputs and outputs, setting the **number of epochs to 50**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Nkb5HbGCssZg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "489/489 [==============================] - 34s 62ms/step - loss: 7.2271 - accuracy: 0.0626 - val_loss: 6.8431 - val_accuracy: 0.0685\n",
      "Epoch 2/50\n",
      "489/489 [==============================] - 30s 61ms/step - loss: 6.6373 - accuracy: 0.0800 - val_loss: 6.6754 - val_accuracy: 0.0956\n",
      "Epoch 3/50\n",
      "489/489 [==============================] - 27s 55ms/step - loss: 6.3379 - accuracy: 0.0989 - val_loss: 6.5482 - val_accuracy: 0.1241\n",
      "Epoch 4/50\n",
      "489/489 [==============================] - 27s 55ms/step - loss: 6.1101 - accuracy: 0.1288 - val_loss: 6.4454 - val_accuracy: 0.1497\n",
      "Epoch 5/50\n",
      "489/489 [==============================] - 26s 53ms/step - loss: 5.9110 - accuracy: 0.1526 - val_loss: 6.3772 - val_accuracy: 0.1704\n",
      "Epoch 6/50\n",
      "489/489 [==============================] - 26s 53ms/step - loss: 5.7229 - accuracy: 0.1733 - val_loss: 6.3213 - val_accuracy: 0.1923\n",
      "Epoch 7/50\n",
      "489/489 [==============================] - 27s 55ms/step - loss: 5.5403 - accuracy: 0.1906 - val_loss: 6.3035 - val_accuracy: 0.2136\n",
      "Epoch 8/50\n",
      "489/489 [==============================] - 26s 54ms/step - loss: 5.3696 - accuracy: 0.2086 - val_loss: 6.2727 - val_accuracy: 0.2265\n",
      "Epoch 9/50\n",
      "489/489 [==============================] - 27s 55ms/step - loss: 5.2115 - accuracy: 0.2209 - val_loss: 6.3062 - val_accuracy: 0.2265\n",
      "Epoch 10/50\n",
      "489/489 [==============================] - 27s 55ms/step - loss: 5.0607 - accuracy: 0.2297 - val_loss: 6.3234 - val_accuracy: 0.2294\n",
      "Epoch 11/50\n",
      "489/489 [==============================] - 27s 56ms/step - loss: 4.9119 - accuracy: 0.2390 - val_loss: 6.3570 - val_accuracy: 0.2346\n",
      "Epoch 12/50\n",
      "489/489 [==============================] - 27s 55ms/step - loss: 4.7711 - accuracy: 0.2462 - val_loss: 6.4275 - val_accuracy: 0.2404\n",
      "Epoch 13/50\n",
      "489/489 [==============================] - 28s 56ms/step - loss: 4.6389 - accuracy: 0.2536 - val_loss: 6.4662 - val_accuracy: 0.2398\n",
      "Epoch 14/50\n",
      "489/489 [==============================] - 27s 56ms/step - loss: 4.5077 - accuracy: 0.2617 - val_loss: 6.5244 - val_accuracy: 0.2441\n",
      "Epoch 15/50\n",
      "489/489 [==============================] - 28s 56ms/step - loss: 4.3821 - accuracy: 0.2680 - val_loss: 6.6036 - val_accuracy: 0.2432\n",
      "Epoch 16/50\n",
      "489/489 [==============================] - 27s 56ms/step - loss: 4.2603 - accuracy: 0.2760 - val_loss: 6.6679 - val_accuracy: 0.2467\n",
      "Epoch 17/50\n",
      "489/489 [==============================] - 28s 57ms/step - loss: 4.1430 - accuracy: 0.2810 - val_loss: 6.7362 - val_accuracy: 0.2559\n",
      "Epoch 18/50\n",
      "489/489 [==============================] - 29s 59ms/step - loss: 4.0240 - accuracy: 0.2901 - val_loss: 6.8341 - val_accuracy: 0.2579\n",
      "Epoch 19/50\n",
      "489/489 [==============================] - 28s 57ms/step - loss: 3.9131 - accuracy: 0.2977 - val_loss: 6.9218 - val_accuracy: 0.2579\n",
      "Epoch 20/50\n",
      "489/489 [==============================] - 28s 57ms/step - loss: 3.8018 - accuracy: 0.3063 - val_loss: 7.0296 - val_accuracy: 0.2573\n",
      "Epoch 21/50\n",
      "489/489 [==============================] - 28s 57ms/step - loss: 3.6960 - accuracy: 0.3179 - val_loss: 7.1158 - val_accuracy: 0.2608\n",
      "Epoch 22/50\n",
      "489/489 [==============================] - 28s 57ms/step - loss: 3.5927 - accuracy: 0.3314 - val_loss: 7.2375 - val_accuracy: 0.2585\n",
      "Epoch 23/50\n",
      "489/489 [==============================] - 28s 57ms/step - loss: 3.4920 - accuracy: 0.3474 - val_loss: 7.3369 - val_accuracy: 0.2576\n",
      "Epoch 24/50\n",
      "489/489 [==============================] - 28s 58ms/step - loss: 3.3958 - accuracy: 0.3605 - val_loss: 7.4497 - val_accuracy: 0.2596\n",
      "Epoch 25/50\n",
      "489/489 [==============================] - 28s 57ms/step - loss: 3.3049 - accuracy: 0.3744 - val_loss: 7.5484 - val_accuracy: 0.2614\n",
      "Epoch 26/50\n",
      "489/489 [==============================] - 28s 57ms/step - loss: 3.2158 - accuracy: 0.3866 - val_loss: 7.6605 - val_accuracy: 0.2619\n",
      "Epoch 27/50\n",
      "489/489 [==============================] - 28s 57ms/step - loss: 3.1321 - accuracy: 0.4021 - val_loss: 7.7466 - val_accuracy: 0.2614\n",
      "Epoch 28/50\n",
      "489/489 [==============================] - 28s 57ms/step - loss: 3.0519 - accuracy: 0.4141 - val_loss: 7.8642 - val_accuracy: 0.2619\n",
      "Epoch 29/50\n",
      "489/489 [==============================] - 28s 57ms/step - loss: 2.9764 - accuracy: 0.4270 - val_loss: 7.9757 - val_accuracy: 0.2582\n",
      "Epoch 30/50\n",
      "489/489 [==============================] - 28s 58ms/step - loss: 2.9034 - accuracy: 0.4382 - val_loss: 8.0935 - val_accuracy: 0.2582\n",
      "Epoch 31/50\n",
      "489/489 [==============================] - 29s 58ms/step - loss: 2.8344 - accuracy: 0.4505 - val_loss: 8.1655 - val_accuracy: 0.2585\n",
      "Epoch 32/50\n",
      "489/489 [==============================] - 29s 58ms/step - loss: 2.7676 - accuracy: 0.4614 - val_loss: 8.2853 - val_accuracy: 0.2568\n",
      "Epoch 33/50\n",
      "489/489 [==============================] - 29s 60ms/step - loss: 2.7049 - accuracy: 0.4705 - val_loss: 8.3969 - val_accuracy: 0.2596\n",
      "Epoch 34/50\n",
      "489/489 [==============================] - 30s 61ms/step - loss: 2.6446 - accuracy: 0.4817 - val_loss: 8.4754 - val_accuracy: 0.2545\n",
      "Epoch 35/50\n",
      "489/489 [==============================] - 30s 62ms/step - loss: 2.5858 - accuracy: 0.4905 - val_loss: 8.5711 - val_accuracy: 0.2605\n",
      "Epoch 36/50\n",
      "489/489 [==============================] - 30s 61ms/step - loss: 2.5283 - accuracy: 0.4994 - val_loss: 8.6709 - val_accuracy: 0.2619\n",
      "Epoch 37/50\n",
      "489/489 [==============================] - 30s 61ms/step - loss: 2.4771 - accuracy: 0.5104 - val_loss: 8.7497 - val_accuracy: 0.2579\n",
      "Epoch 38/50\n",
      "489/489 [==============================] - 30s 62ms/step - loss: 2.4243 - accuracy: 0.5182 - val_loss: 8.8540 - val_accuracy: 0.2585\n",
      "Epoch 39/50\n",
      "489/489 [==============================] - 30s 61ms/step - loss: 2.3735 - accuracy: 0.5271 - val_loss: 8.9466 - val_accuracy: 0.2588\n",
      "Epoch 40/50\n",
      "489/489 [==============================] - 30s 61ms/step - loss: 2.3269 - accuracy: 0.5354 - val_loss: 8.9882 - val_accuracy: 0.2599\n",
      "Epoch 41/50\n",
      "489/489 [==============================] - 33s 67ms/step - loss: 2.2808 - accuracy: 0.5432 - val_loss: 9.1005 - val_accuracy: 0.2599\n",
      "Epoch 42/50\n",
      "489/489 [==============================] - 27s 55ms/step - loss: 2.2371 - accuracy: 0.5515 - val_loss: 9.1966 - val_accuracy: 0.2571\n",
      "Epoch 43/50\n",
      "489/489 [==============================] - 27s 54ms/step - loss: 2.1907 - accuracy: 0.5600 - val_loss: 9.2655 - val_accuracy: 0.2602\n",
      "Epoch 44/50\n",
      "489/489 [==============================] - 28s 57ms/step - loss: 2.1484 - accuracy: 0.5685 - val_loss: 9.3404 - val_accuracy: 0.2605\n",
      "Epoch 45/50\n",
      "489/489 [==============================] - 28s 57ms/step - loss: 2.1074 - accuracy: 0.5757 - val_loss: 9.3981 - val_accuracy: 0.2602\n",
      "Epoch 46/50\n",
      "489/489 [==============================] - 29s 60ms/step - loss: 2.0683 - accuracy: 0.5811 - val_loss: 9.4888 - val_accuracy: 0.2576\n",
      "Epoch 47/50\n",
      "489/489 [==============================] - 29s 59ms/step - loss: 2.0298 - accuracy: 0.5890 - val_loss: 9.5984 - val_accuracy: 0.2619\n",
      "Epoch 48/50\n",
      "489/489 [==============================] - 28s 57ms/step - loss: 1.9934 - accuracy: 0.5982 - val_loss: 9.6809 - val_accuracy: 0.2614\n",
      "Epoch 49/50\n",
      "489/489 [==============================] - 31s 63ms/step - loss: 1.9582 - accuracy: 0.6034 - val_loss: 9.7374 - val_accuracy: 0.2599\n",
      "Epoch 50/50\n",
      "489/489 [==============================] - 30s 60ms/step - loss: 1.9216 - accuracy: 0.6104 - val_loss: 9.8018 - val_accuracy: 0.2576\n",
      "[info] Training complete.\n"
     ]
    }
   ],
   "source": [
    "# Train the slogan generator -next-word model\n",
    "# Inputs:\n",
    "#   X_gen : padded prefixes with shape (num_samples, max_seq_len - 1)\n",
    "#   y_gen : one-hot next-token targets with shape (num_samples, total_words)\n",
    "# Spec:\n",
    "#    epochs = 50 (per instructions)\n",
    "#    reasonable batch size (e.g., 64)\n",
    "#    small validation split for quick overfitting signal\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Basic sanity checks\n",
    "assert \"X_gen\" in globals() and \"y_gen\" in globals(), \"X_gen / y_gen not found. Build them in previous steps.\"\n",
    "assert X_gen.shape[0] == y_gen.shape[0], \"Mismatched sample counts between X_gen and y_gen.\"\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = gen_model.fit(\n",
    "    X_gen,\n",
    "    y_gen,\n",
    "    epochs=50,           \n",
    "    batch_size=64,\n",
    "    validation_split=0.1,\n",
    "    callbacks=None,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"[info] Training complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ln30w1SovlKy"
   },
   "source": [
    "We will now define a function called `generate_slogan` which will generate a slogan by predicting one word at a time based on a given starting phrase (the `seed_text`). This function will do this using our trained model, `gen_model`.\n",
    "\n",
    "Here is a breakdown of how the algorithm works:  \n",
    "\n",
    "Let us assume the dictionary mapping words to unique indices, `tokenizer.word_index`, looks like this:\n",
    "\n",
    "> `{'computer': 1, 'hardware': 2, 'taking': 3, 'care': 4, 'of': 5}`\n",
    "\n",
    "If the model's predicted index for the next word is 3 (`predicted_index = 3`), the loop will:\n",
    "\n",
    "> Check 'computer' (index 1) → No match  \n",
    "> Check 'hardware' (index 2) → No match  \n",
    "> Check 'taking' (index 3) → Match found!  \n",
    "> Assign output_word = \"taking\" and exit the loop.  \n",
    "\n",
    "The `output_word` will be appended to the `seed_text`, and the process will continue to add words to the `seed_text` until we have reached the maximum number of words **or** an invalid prediction occurs.  \n",
    "\n",
    "Carefully follow the code below and complete the missing parts as guided by the comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "oQzoxk5avlXB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "insurance insurance broker in the in indies ia dealer in london ny needs and central estate estate estate estate estate electronics\n"
     ]
    }
   ],
   "source": [
    "# Next-word slogan generation using the trained gen_model\n",
    "# Strategy:\n",
    "#    Encode the current seed_text → token IDs → pad to (max_seq_len-1)\n",
    "#    gen_model predicts a probability distribution over the vocab\n",
    "#    pick the most likely next token (argmax) and map it back to a word\n",
    "#    append that word to seed_text and repeat up to max_words steps\n",
    "\n",
    "\n",
    "def generate_slogan(seed_text: str, max_words: int = 20) -> str:\n",
    "    \"\"\"Greedy next-word generation using the trained LSTM language model.\"\"\"\n",
    "    text = str(seed_text).strip().lower()\n",
    "\n",
    "    for _ in range(max_words):\n",
    "        #  Tokenise and pad the current prefix\n",
    "        token_list = tokenizer.texts_to_sequences([text])[0]         \n",
    "        # shape -> (1, max_seq_len-1)\n",
    "        token_list = pad_sequences([token_list], maxlen=max_seq_len-1,  \n",
    "                                   padding=\"pre\")\n",
    "\n",
    "        #  Predict next-word distribution over the vocabulary\n",
    "        preds = gen_model.predict(token_list, verbose=0)                \n",
    "\n",
    "        #  Choose the most likely next token \n",
    "        predicted_index = int(np.argmax(preds, axis=-1)[0])            \n",
    "\n",
    "        # Map the predicted index back to a word\n",
    "        output_word = None\n",
    "\n",
    "        # Skip invalid/padding index (0) if it appears\n",
    "        if predicted_index == 0:\n",
    "            break\n",
    "\n",
    "        # Linear search over tokenizer.word_index to find the word for this index\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted_index:\n",
    "                output_word = word\n",
    "                break\n",
    "\n",
    "        # If I couldn't find a matching word, stop generation\n",
    "        if output_word is None:\n",
    "            break\n",
    "\n",
    "        # Append predicted word to the running text and continue\n",
    "        text = f\"{text} {output_word}\"\n",
    "\n",
    "    return text\n",
    "\n",
    "# Example (after training):\n",
    "print(generate_slogan(\"insurance\"))  # or any seed phrase present in your data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AHAlDcCtwPlu"
   },
   "source": [
    "## Training Data for Slogan Classifier\n",
    "\n",
    "We will now prepare the data we will use to train our classifier. For our classifier, the inputs will come from the `processed_slogans` column of our DataFrame, `df`. The outputs will be the different industry categories under the `industry` column.\n",
    "\n",
    "In the code cell below, extract the unique values from the `industry` column in the DataFrame and store these in a variable called **industries**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "-DHQNp-uwcTL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Found 142 unique industries in column 'industry'.\n",
      "['accounting', 'airlines/aviation', 'alternative medicine', 'animation', 'apparel & fashion', 'architecture & planning', 'arts and crafts', 'automotive', 'aviation & aerospace', 'banking']\n"
     ]
    }
   ],
   "source": [
    "#Collect unique industry labels for the classifier \n",
    "# Goal: extract the distinct categories from the industry column and store them in industries.\n",
    "\n",
    "\n",
    "#  Locate the industry column- handle common name variants defensivel\n",
    "IND_COL = None\n",
    "for cand in [\"industry\", \"Industry\", \"sector\", \"category\"]:\n",
    "    if cand in df.columns:\n",
    "        IND_COL = cand\n",
    "        break\n",
    "if IND_COL is None:\n",
    "    raise KeyError(\"Could not find an industry column. Expected one of: 'industry', 'Industry', 'sector', 'category'.\")\n",
    "\n",
    "# Extract unique labels, drop missing, normalise whitespace\n",
    "industries = (\n",
    "    df[IND_COL]\n",
    "    .dropna()\n",
    "    .astype(str)\n",
    "    .map(lambda s: s.strip())\n",
    "    .unique()\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "# Sort for stable ordering\n",
    "industries = sorted(industries)\n",
    "\n",
    "print(f\"[info] Found {len(industries)} unique industries in column '{IND_COL}'.\")\n",
    "print(industries[:10])  # peek at a few\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OVRLAurq0fz0"
   },
   "source": [
    "Create a dictionary called `industry_to_index` where each unique industry is mapped to a unique index starting from 0.\n",
    "\n",
    "*Hint: Use the `enumerate()` function.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ifWI9iRm0gAQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Number of classes: 142\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('accounting', 0),\n",
       " ('airlines/aviation', 1),\n",
       " ('alternative medicine', 2),\n",
       " ('animation', 3),\n",
       " ('apparel & fashion', 4),\n",
       " ('architecture & planning', 5),\n",
       " ('arts and crafts', 6),\n",
       " ('automotive', 7),\n",
       " ('aviation & aerospace', 8),\n",
       " ('banking', 9)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map each unique industry to a numeric ID starting at 0 \n",
    "\n",
    "if \"industries\" not in globals() or not isinstance(industries, list) or len(industries) == 0:\n",
    "    raise ValueError(\"industries is missing or empty. Run the previous cell that builds the unique industry list.\")\n",
    "\n",
    "# Create mapping: industry -> index (0, 1, 2, ...)\n",
    "industry_to_index = {name: idx for idx, name in enumerate(industries)}\n",
    "\n",
    "# reverse mapping (handy for decoding predictions later)\n",
    "index_to_industry = {idx: name for name, idx in industry_to_index.items()}\n",
    "\n",
    "print(f\"[info] Number of classes: {len(industry_to_index)}\")\n",
    "# Peek at the first few items to confirm ordering\n",
    "list(industry_to_index.items())[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pkr04p3g0vzj"
   },
   "source": [
    "Create a new column `industry_index` in your DataFrame by mapping the `industry` column to the indices using the `industry_to_index` dictionary.\n",
    "\n",
    "*Hint: Use the  `map()` function.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "bRT8zwHc0v7F"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Added 'industry_index' column.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>industry</th>\n",
       "      <th>industry_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>computer hardware</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>health, wellness and fitness</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>internet</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>internet</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>financial services</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       industry  industry_index\n",
       "0             computer hardware              21\n",
       "1  health, wellness and fitness              52\n",
       "2                      internet              65\n",
       "3                      internet              65\n",
       "4            financial services              41"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create 'industry_index' by mapping industry labels to integer IDs \n",
    "\n",
    "# Reuse / detect the industry column name\n",
    "IND_COL = None\n",
    "for cand in [\"industry\", \"Industry\", \"sector\", \"category\"]:\n",
    "    if cand in df.columns:\n",
    "        IND_COL = cand\n",
    "        break\n",
    "if IND_COL is None:\n",
    "    raise KeyError(\"Industry column not found. Expected one of: 'industry', 'Industry', 'sector', 'category'.\")\n",
    "\n",
    "# Sanity check that the mapping dict exists\n",
    "if \"industry_to_index\" not in globals() or not isinstance(industry_to_index, dict) or len(industry_to_index) == 0:\n",
    "    raise ValueError(\"industry_to_index is missing or empty. Build it from the unique industries first.\")\n",
    "\n",
    "#  Map labels → indices; rows with unseen/missing labels become NaN, so handle them\n",
    "df[\"industry_index\"] = (\n",
    "    df[IND_COL]\n",
    "    .map(lambda s: str(s).strip() if pd.notna(s) else s)  # normalise whitespace\n",
    "    .map(industry_to_index)                                # map to ints via dictionary\n",
    ")\n",
    "\n",
    "#  if you want a sentinel for missing/unmapped labels, uncomment:\n",
    "df[\"industry_index\"] = df[\"industry_index\"].fillna(-1).astype(int)\n",
    "\n",
    "print(\"[info] Added 'industry_index' column.\")\n",
    "df[[IND_COL, \"industry_index\"]].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bICgqYlkCHU2"
   },
   "source": [
    "Split the DataFrame `df` into training and testing sets, setting aside 20% of the data for the test set. Be sure to set the parameter `stratify=df[\"industry_index\"]`. This ensures that both sets have the same proportion of each class (industry) as in the original dataset, resulting in balanced datasets. Call the training DataFrame `df_train` and the testing DataFrame `df_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "4PdSJ2gYC9Tp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[warn] Dropping 6 rare class(es) with <2 samples to enable stratified split.\n",
      "[info] Train shape: (4272, 15), Test shape: (1068, 15)\n",
      "[info] Class proportion check (first 10 rows):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_prop</th>\n",
       "      <th>test_prop</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>industry_index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.013343</td>\n",
       "      <td>0.013109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.002575</td>\n",
       "      <td>0.002809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000468</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000702</td>\n",
       "      <td>0.000936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.010768</td>\n",
       "      <td>0.011236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.006086</td>\n",
       "      <td>0.005618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000702</td>\n",
       "      <td>0.000936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.029494</td>\n",
       "      <td>0.029026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.003043</td>\n",
       "      <td>0.002809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.002341</td>\n",
       "      <td>0.002809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                train_prop  test_prop\n",
       "industry_index                       \n",
       "0                 0.013343   0.013109\n",
       "1                 0.002575   0.002809\n",
       "2                 0.000468        NaN\n",
       "3                 0.000702   0.000936\n",
       "4                 0.010768   0.011236\n",
       "5                 0.006086   0.005618\n",
       "6                 0.000702   0.000936\n",
       "7                 0.029494   0.029026\n",
       "8                 0.003043   0.002809\n",
       "9                 0.002341   0.002809"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train / Test split with class balance (stratified) \n",
    "\n",
    "\n",
    "#  Keep only rows with a valid class index\n",
    "df_clean = df.dropna(subset=[\"industry_index\"]).copy()\n",
    "\n",
    "#   Ensure every class has at least 2 samples so stratify can split\n",
    "#   If a class has only 1 row, stratified split will fail. I’ll drop such classes for now.\n",
    "counts = df_clean[\"industry_index\"].value_counts()\n",
    "ok_classes = counts[counts >= 2].index\n",
    "dropped_classes = set(df_clean[\"industry_index\"].unique()) - set(ok_classes)\n",
    "if dropped_classes:\n",
    "    print(f\"[warn] Dropping {len(dropped_classes)} rare class(es) with <2 samples to enable stratified split.\")\n",
    "    df_clean = df_clean[df_clean[\"industry_index\"].isin(ok_classes)].copy()\n",
    "\n",
    "#  Perform stratified 80/20 split\n",
    "df_train, df_test = train_test_split(\n",
    "    df_clean,\n",
    "    test_size=0.20,\n",
    "    stratify=df_clean[\"industry_index\"],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"[info] Train shape: {df_train.shape}, Test shape: {df_test.shape}\")\n",
    "\n",
    "# Quick check: class proportions are similar in both splits\n",
    "train_props = df_train[\"industry_index\"].value_counts(normalize=True).sort_index()\n",
    "test_props  = df_test[\"industry_index\"].value_counts(normalize=True).sort_index()\n",
    "check = pd.DataFrame({\"train_prop\": train_props, \"test_prop\": test_props})\n",
    "print(\"[info] Class proportion check (first 10 rows):\")\n",
    "display(check.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RQ43WVLD3F9t"
   },
   "source": [
    "Our classifier will use padded slogan sequences as inputs, similar to input sequences used for the slogan generator. The difference is we will not use sequences that get progressively longer, but instead we will use **complete slogans**. This is because our classifier does not need to learn how to predict what word comes next. It needs the full context of a slogan to learn how to accurately predict the industry.  \n",
    "\n",
    "The next steps will walk you through how to create these sequences.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_4MDmlZ349bC"
   },
   "source": [
    "We previously created and fitted a `Tokenizer` object called `tokenizer` while preparing data for the slogan generator. Now, we will reuse it to convert words into numerical indices.  \n",
    "\n",
    "In the code cell below, use the `texts_to_sequences()` **method** of `tokenizer` to transform the `processed_slogan` column in **both** the `df_train` and `df_test` DataFrames into sequences of numerical indices. Store the results in variables named `X_train` and `X_test`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Ce5ro9jV3GGs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Train samples: 4272, Test samples: 1068\n",
      "[info] Example train sequence (first 1): [443, 418, 4049, 852, 1, 2111, 153] ...\n"
     ]
    }
   ],
   "source": [
    "# --- Convert full slogans to integer sequences for the CLASSIFIER \n",
    "# I reuse the already-fitted 'tokenizer' from the generator step.\n",
    "# Output:\n",
    "#   X_train, X_test  -> lists of lists (token IDs), one per slogan. I'll pad them in the next step.\n",
    "\n",
    "# 1) Sanity checks\n",
    "if \"tokenizer\" not in globals():\n",
    "    raise NameError(\"'tokenizer' not found. Fit the Tokenizer in the generator section first.\")\n",
    "if \"processed_slogan\" not in df_train.columns or \"processed_slogan\" not in df_test.columns:\n",
    "    raise KeyError(\"Missing 'processed_slogan' column in df_train/df_test. Run preprocessing steps first.\")\n",
    "\n",
    "# 2) Convert text → sequences of token IDs\n",
    "#    (I keep them as variable-length lists here; padding comes next.)\n",
    "X_train = tokenizer.texts_to_sequences(df_train[\"processed_slogan\"].astype(str).tolist())\n",
    "X_test  = tokenizer.texts_to_sequences(df_test[\"processed_slogan\"].astype(str).tolist())\n",
    "\n",
    "# 3) Quick sanity prints\n",
    "print(f\"[info] Train samples: {len(X_train)}, Test samples: {len(X_test)}\")\n",
    "print(f\"[info] Example train sequence (first 1): {X_train[0][:20]} ...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "klUEvtcY8DHW"
   },
   "source": [
    "The slogan sequences are of varying lengths. We will need to pad them the same way we did to the input sequences for the slogan generator. The `pad_sequences()` function can ensure the sequences in `slogan_sequences` have the same length.  \n",
    "\n",
    "In the code cell below, use the `pad_sequences()` function to standardise the `slogan_sequences` lengths. Set the `maxlen` parameter to `max_seq_len`, the `padding` parameter to 0, and assign the resulting padded sequences to the same variables, `X_train` and `X_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "gu30y1hO8DR8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] X_train shape: (4272, 15)\n",
      "[info] X_test  shape: (1068, 15)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0,    0,    0,    0,    0,  443,  418, 4049,\n",
       "         852,    1, 2111,  153],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "        1568, 1798, 2579, 2465]], dtype=int32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pad classifier inputs to a fixed length \n",
    "# Goal: make all sequences the same length so they can batch through the LSTM/Dense stack.\n",
    "# I’ll:\n",
    "#    pad on the LEFT (\"pre\") so the most recent tokens line up at the end\n",
    "#    use zero as the pad value (Keras default)\n",
    "#    standardise to max_seq_len (full slogan length for classifier)\n",
    "\n",
    "\n",
    "# Sanity checks\n",
    "if \"max_seq_len\" not in globals() or not isinstance(max_seq_len, int) or max_seq_len < 1:\n",
    "    raise ValueError(\"max_seq_len is missing/invalid. Compute it from input_sequences earlier.\")\n",
    "\n",
    "X_train = pad_sequences(\n",
    "    X_train,\n",
    "    maxlen=max_seq_len,   # target length for full-slogan inputs\n",
    "    padding=\"pre\",        # pad with zeros on the left - pad value defaults to 0\n",
    "    truncating=\"pre\",     # if anything is longer, trim from the left\n",
    "    value=0\n",
    ")\n",
    "\n",
    "X_test = pad_sequences(\n",
    "    X_test,\n",
    "    maxlen=max_seq_len,\n",
    "    padding=\"pre\",\n",
    "    truncating=\"pre\",\n",
    "    value=0\n",
    ")\n",
    "\n",
    "print(f\"[info] X_train shape: {X_train.shape}\")\n",
    "print(f\"[info] X_test  shape: {X_test.shape}\")\n",
    "# Peek at a couple of rows\n",
    "X_train[:2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uIpa6O7MANYO"
   },
   "source": [
    "We have successfully created training and testing inputs for our model. Now, we will create the outputs - industry categories.\n",
    "\n",
    " In the code cell that follows, use `tf.keras.utils.to_categorical()` to apply one-hot encoding to the `industry_index` column of **both** `df_train` and `df_test` DataFrames. Assign the results to a variables named `y_train` and `y_test`.\n",
    "\n",
    " *Hint: set the `num_classes` parameter to the total number of industries in the DataFrame. The `industries` variable can be used to find this value.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "oOGzb_8BANpE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] y_train shape: (4272, 142)  (num_classes=142)\n",
      "[info] y_test  shape: (1068, 142)\n",
      "[info] First train label index: 43\n"
     ]
    }
   ],
   "source": [
    "# One-hot encode industry labels for the CLASSIFIER \n",
    "# Goal:\n",
    "#   y_train, y_test -> one-hot arrays with shape (num_samples, num_classes)\n",
    "#   where num_classes = number of unique industries.\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Sanity checks\n",
    "if \"industries\" not in globals() or not isinstance(industries, list) or len(industries) == 0:\n",
    "    raise ValueError(\"industries is missing or empty. Build it from the unique industry list first.\")\n",
    "if \"industry_index\" not in df_train.columns or \"industry_index\" not in df_test.columns:\n",
    "    raise KeyError(\"Missing 'industry_index' in df_train/df_test. Map labels to indices before this step.\")\n",
    "\n",
    "num_classes = len(industries)\n",
    "\n",
    "# Extract label indices as integer arrays\n",
    "y_train_idx = df_train[\"industry_index\"].to_numpy(dtype=np.int32)\n",
    "y_test_idx  = df_test[\"industry_index\"].to_numpy(dtype=np.int32)\n",
    "\n",
    "#  One-hot encode\n",
    "y_train = to_categorical(y_train_idx, num_classes=num_classes, dtype=\"float32\")\n",
    "y_test  = to_categorical(y_test_idx,  num_classes=num_classes, dtype=\"float32\")\n",
    "\n",
    "print(f\"[info] y_train shape: {y_train.shape}  (num_classes={num_classes})\")\n",
    "print(f\"[info] y_test  shape: {y_test.shape}\")\n",
    "# Quick check: show the class index of the first sample\n",
    "print(f\"[info] First train label index: {int(y_train_idx[0])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QlPKC7v-n7dM"
   },
   "source": [
    "## Slogan Classifier Architecture\n",
    "\n",
    "Configure the LSTM classifier following these steps:  \n",
    "\n",
    "\n",
    "1. Create a Sequential model:  \n",
    "   Use `tf.keras.models.Sequential()` to create a sequential model. This model will consist of an embedding layer, two LSTM layers, and a dense output layer.\n",
    "\n",
    "2. Add an embedding layer which will convert words into dense vector representations. Configure this layer with:\n",
    "   > * `total_words` as the vocabulary size.\n",
    "   > * 100 as the embedding dimension.\n",
    "   > * `max_seq_len` as the `input_length` (this is the length of the slogans).\n",
    "\n",
    "3. Add the first LSTM layer. Configure it with:\n",
    "   > * 150 units.\n",
    "   > * Set `return_sequences` to `True` to ensure the layer outputs sequences for the next LSTM layer.\n",
    "\n",
    "4. Add the second LSTM layer which will process the output from the previous LSTM layer. Configure it with:\n",
    "   > * 100 units.\n",
    "   > * No need to set `return_sequences` here (it is the final LSTM layer).\n",
    "\n",
    "5. Add the dense output layer which will classify the data into industries. Configure it with:\n",
    "   > * The number of unique industries as the number of units.\n",
    "   > * The `softmax` activation function to get probabilities for each class (industry).\n",
    "\n",
    "6. Use `Sequential` to arrange all layers in the correct order and complete the architecture of the LSTM model called **class_model**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "JIHaUMT4oC4Y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 15, 100)           604600    \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 15, 150)           150600    \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 100)               100400    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 142)               14342     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 869942 (3.32 MB)\n",
      "Trainable params: 869942 (3.32 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# --- Slogan CLASSIFIER: LSTM architecture (Sequential) ---\n",
    "# Spec:\n",
    "#    Embedding(vocab_size=total_words, embed_dim=100, input_length=max_seq_len)\n",
    "#    LSTM(150, return_sequences=True)\n",
    "#    LSTM(100)\n",
    "#    Dense(num_classes, activation=\"softmax\")\n",
    "#\n",
    "# Notes:\n",
    "#   - total_words  : tokenizer vocabulary size (len(tokenizer.word_index) + 1)\n",
    "#   - max_seq_len  : padded slogan length for classifier inputs\n",
    "#   - num_classes  : number of unique industries (len(industries))\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Sanity checks to catch earlier setup issues\n",
    "assert \"total_words\" in globals() and isinstance(total_words, int) and total_words > 0, \\\n",
    "    \"total_words missing/invalid. Re-run the tokeniser step.\"\n",
    "assert \"max_seq_len\" in globals() and isinstance(max_seq_len, int) and max_seq_len > 0, \\\n",
    "    \"max_seq_len missing/invalid. Compute it before building the classifier.\"\n",
    "num_classes = len(industries)\n",
    "assert num_classes > 1, \"Need at least 2 industries for a multi-class classifier.\"\n",
    "\n",
    "class_model = Sequential([\n",
    "    # Word ID -> dense vector representation\n",
    "    Embedding(\n",
    "        input_dim=total_words,   # vocabulary size\n",
    "        output_dim=100,          # embedding dimension (per brief)\n",
    "        input_length=max_seq_len # full slogan length\n",
    "    ),\n",
    "    # First recurrent layer; keep full sequence for stacking\n",
    "    LSTM(150, return_sequences=True),\n",
    "    # Second recurrent layer; produce final sequence encoding\n",
    "    LSTM(100),\n",
    "    # Predict industry class distribution\n",
    "    Dense(num_classes, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# Inspect shapes/parameter counts\n",
    "class_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SlMle0UzGzG_"
   },
   "source": [
    "In the code cell below, compile `class_model` using `categorical_crossentropy` loss, an Adam optimiser, and an appropriate metric of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "ltBuXv3TGzPm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] class_model compiled: loss=categorical_crossentropy, optimizer=Adam(1e-3), metrics=['accuracy']\n"
     ]
    }
   ],
   "source": [
    "#  Compile the classifier model \n",
    "# Multi-class problem → categorical_crossentropy is appropriate with one-hot targets.\n",
    "# Adam is a solid default optimiser. Track accuracy for a quick performance signal.\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "class_model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=Adam(learning_rate=1e-3),\n",
    "    metrics=[\"accuracy\"]   # you could also add: [\"accuracy\", \"top_k_categorical_accuracy\"]\n",
    ")\n",
    "\n",
    "print(\"[info] class_model compiled: loss=categorical_crossentropy, optimizer=Adam(1e-3), metrics=['accuracy']\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Zvuqmg2HA3e"
   },
   "source": [
    "## Slogan Classification & Evaluation\n",
    "\n",
    "In the code cell that follows, fit the compiled model on the inputs and outputs, setting **the number of epochs to 50**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "hZyjCegNHTrc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "67/67 [==============================] - 8s 56ms/step - loss: 4.4662 - accuracy: 0.0761 - val_loss: 4.2787 - val_accuracy: 0.0843\n",
      "Epoch 2/50\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 4.2882 - accuracy: 0.0847 - val_loss: 4.2705 - val_accuracy: 0.0843\n",
      "Epoch 3/50\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 4.2756 - accuracy: 0.0847 - val_loss: 4.2699 - val_accuracy: 0.0843\n",
      "Epoch 4/50\n",
      "67/67 [==============================] - 3s 43ms/step - loss: 4.2627 - accuracy: 0.0847 - val_loss: 4.2425 - val_accuracy: 0.0843\n",
      "Epoch 5/50\n",
      "67/67 [==============================] - 3s 43ms/step - loss: 4.1090 - accuracy: 0.0983 - val_loss: 4.1060 - val_accuracy: 0.0908\n",
      "Epoch 6/50\n",
      "67/67 [==============================] - 3s 45ms/step - loss: 3.8320 - accuracy: 0.1463 - val_loss: 4.0179 - val_accuracy: 0.1320\n",
      "Epoch 7/50\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 3.5058 - accuracy: 0.2095 - val_loss: 3.9510 - val_accuracy: 0.1573\n",
      "Epoch 8/50\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 3.1398 - accuracy: 0.2802 - val_loss: 3.9177 - val_accuracy: 0.1770\n",
      "Epoch 9/50\n",
      "67/67 [==============================] - 3s 45ms/step - loss: 2.7901 - accuracy: 0.3570 - val_loss: 3.9675 - val_accuracy: 0.1788\n",
      "Epoch 10/50\n",
      "67/67 [==============================] - 3s 46ms/step - loss: 2.4783 - accuracy: 0.4309 - val_loss: 3.9717 - val_accuracy: 0.1873\n",
      "Epoch 11/50\n",
      "67/67 [==============================] - 3s 45ms/step - loss: 2.1892 - accuracy: 0.4906 - val_loss: 4.1076 - val_accuracy: 0.1948\n",
      "Epoch 12/50\n",
      "67/67 [==============================] - 3s 45ms/step - loss: 1.9230 - accuracy: 0.5555 - val_loss: 4.1820 - val_accuracy: 0.2013\n",
      "Epoch 13/50\n",
      "67/67 [==============================] - 3s 49ms/step - loss: 1.6857 - accuracy: 0.6196 - val_loss: 4.2628 - val_accuracy: 0.1929\n",
      "Epoch 14/50\n",
      "67/67 [==============================] - 3s 49ms/step - loss: 1.4763 - accuracy: 0.6749 - val_loss: 4.3986 - val_accuracy: 0.2004\n",
      "Epoch 15/50\n",
      "67/67 [==============================] - 3s 48ms/step - loss: 1.3013 - accuracy: 0.7179 - val_loss: 4.4929 - val_accuracy: 0.1957\n",
      "Epoch 16/50\n",
      "67/67 [==============================] - 3s 50ms/step - loss: 1.1497 - accuracy: 0.7563 - val_loss: 4.6205 - val_accuracy: 0.1788\n",
      "Epoch 17/50\n",
      "67/67 [==============================] - 4s 57ms/step - loss: 1.0062 - accuracy: 0.7968 - val_loss: 4.6691 - val_accuracy: 0.1901\n",
      "Epoch 18/50\n",
      "67/67 [==============================] - 3s 48ms/step - loss: 0.8941 - accuracy: 0.8174 - val_loss: 4.8323 - val_accuracy: 0.1901\n",
      "Epoch 19/50\n",
      "67/67 [==============================] - 3s 52ms/step - loss: 0.7970 - accuracy: 0.8436 - val_loss: 4.9066 - val_accuracy: 0.1835\n",
      "Epoch 20/50\n",
      "67/67 [==============================] - 3s 48ms/step - loss: 0.7068 - accuracy: 0.8663 - val_loss: 5.0325 - val_accuracy: 0.1873\n",
      "Epoch 21/50\n",
      "67/67 [==============================] - 3s 50ms/step - loss: 0.6197 - accuracy: 0.8837 - val_loss: 5.0905 - val_accuracy: 0.1919\n",
      "Epoch 22/50\n",
      "67/67 [==============================] - 4s 53ms/step - loss: 0.5512 - accuracy: 0.8984 - val_loss: 5.1749 - val_accuracy: 0.1891\n",
      "Epoch 23/50\n",
      "67/67 [==============================] - 3s 50ms/step - loss: 0.4892 - accuracy: 0.9146 - val_loss: 5.2848 - val_accuracy: 0.1845\n",
      "Epoch 24/50\n",
      "67/67 [==============================] - 3s 50ms/step - loss: 0.4389 - accuracy: 0.9218 - val_loss: 5.3931 - val_accuracy: 0.1873\n",
      "Epoch 25/50\n",
      "67/67 [==============================] - 3s 50ms/step - loss: 0.3918 - accuracy: 0.9340 - val_loss: 5.4449 - val_accuracy: 0.1788\n",
      "Epoch 26/50\n",
      "67/67 [==============================] - 3s 49ms/step - loss: 0.3548 - accuracy: 0.9405 - val_loss: 5.5712 - val_accuracy: 0.1854\n",
      "Epoch 27/50\n",
      "67/67 [==============================] - 4s 62ms/step - loss: 0.3190 - accuracy: 0.9480 - val_loss: 5.7321 - val_accuracy: 0.1676\n",
      "Epoch 28/50\n",
      "67/67 [==============================] - 4s 62ms/step - loss: 0.2954 - accuracy: 0.9527 - val_loss: 5.6731 - val_accuracy: 0.1788\n",
      "Epoch 29/50\n",
      "67/67 [==============================] - 3s 49ms/step - loss: 0.2653 - accuracy: 0.9555 - val_loss: 5.7871 - val_accuracy: 0.1948\n",
      "Epoch 30/50\n",
      "67/67 [==============================] - 3s 48ms/step - loss: 0.2314 - accuracy: 0.9649 - val_loss: 5.8637 - val_accuracy: 0.1845\n",
      "Epoch 31/50\n",
      "67/67 [==============================] - 3s 49ms/step - loss: 0.2113 - accuracy: 0.9656 - val_loss: 5.8552 - val_accuracy: 0.1873\n",
      "Epoch 32/50\n",
      "67/67 [==============================] - 3s 47ms/step - loss: 0.1907 - accuracy: 0.9719 - val_loss: 5.9794 - val_accuracy: 0.1816\n",
      "Epoch 33/50\n",
      "67/67 [==============================] - 3s 47ms/step - loss: 0.1747 - accuracy: 0.9757 - val_loss: 5.9831 - val_accuracy: 0.1845\n",
      "Epoch 34/50\n",
      "67/67 [==============================] - 3s 48ms/step - loss: 0.1624 - accuracy: 0.9759 - val_loss: 6.1152 - val_accuracy: 0.1732\n",
      "Epoch 35/50\n",
      "67/67 [==============================] - 3s 50ms/step - loss: 0.1480 - accuracy: 0.9792 - val_loss: 6.1306 - val_accuracy: 0.1760\n",
      "Epoch 36/50\n",
      "67/67 [==============================] - 3s 50ms/step - loss: 0.1365 - accuracy: 0.9806 - val_loss: 6.2157 - val_accuracy: 0.1826\n",
      "Epoch 37/50\n",
      "67/67 [==============================] - 3s 49ms/step - loss: 0.1253 - accuracy: 0.9831 - val_loss: 6.2953 - val_accuracy: 0.1779\n",
      "Epoch 38/50\n",
      "67/67 [==============================] - 3s 47ms/step - loss: 0.1168 - accuracy: 0.9810 - val_loss: 6.2361 - val_accuracy: 0.1826\n",
      "Epoch 39/50\n",
      "67/67 [==============================] - 3s 48ms/step - loss: 0.1038 - accuracy: 0.9843 - val_loss: 6.3337 - val_accuracy: 0.1779\n",
      "Epoch 40/50\n",
      "67/67 [==============================] - 3s 50ms/step - loss: 0.0959 - accuracy: 0.9857 - val_loss: 6.3718 - val_accuracy: 0.1788\n",
      "Epoch 41/50\n",
      "67/67 [==============================] - 3s 48ms/step - loss: 0.0863 - accuracy: 0.9885 - val_loss: 6.4217 - val_accuracy: 0.1788\n",
      "Epoch 42/50\n",
      "67/67 [==============================] - 3s 46ms/step - loss: 0.0783 - accuracy: 0.9920 - val_loss: 6.4491 - val_accuracy: 0.1816\n",
      "Epoch 43/50\n",
      "67/67 [==============================] - 3s 52ms/step - loss: 0.0727 - accuracy: 0.9920 - val_loss: 6.5666 - val_accuracy: 0.1845\n",
      "Epoch 44/50\n",
      "67/67 [==============================] - 3s 50ms/step - loss: 0.0697 - accuracy: 0.9913 - val_loss: 6.6085 - val_accuracy: 0.1873\n",
      "Epoch 45/50\n",
      "67/67 [==============================] - 3s 46ms/step - loss: 0.0640 - accuracy: 0.9916 - val_loss: 6.5333 - val_accuracy: 0.1901\n",
      "Epoch 46/50\n",
      "67/67 [==============================] - 3s 50ms/step - loss: 0.0675 - accuracy: 0.9911 - val_loss: 6.6056 - val_accuracy: 0.1845\n",
      "Epoch 47/50\n",
      "67/67 [==============================] - 3s 48ms/step - loss: 0.0632 - accuracy: 0.9918 - val_loss: 6.6601 - val_accuracy: 0.1873\n",
      "Epoch 48/50\n",
      "67/67 [==============================] - 3s 51ms/step - loss: 0.0573 - accuracy: 0.9920 - val_loss: 6.6980 - val_accuracy: 0.1873\n",
      "Epoch 49/50\n",
      "67/67 [==============================] - 3s 49ms/step - loss: 0.0512 - accuracy: 0.9932 - val_loss: 6.7635 - val_accuracy: 0.1901\n",
      "Epoch 50/50\n",
      "67/67 [==============================] - 3s 50ms/step - loss: 0.0465 - accuracy: 0.9939 - val_loss: 6.8055 - val_accuracy: 0.1882\n",
      "[info] Test loss: 6.8055 | Test accuracy: 0.1882\n"
     ]
    }
   ],
   "source": [
    "# --- Train the slogan CLASSIFIER ---\n",
    "# Inputs:\n",
    "#   X_train, y_train : padded full-slogan sequences and one-hot industry labels\n",
    "#   X_test,  y_test  : held-out set for validation during training\n",
    "# Spec:\n",
    "#    epochs = 50 (per instructions)\n",
    "#    batch_size kept modest for stable updates\n",
    "\n",
    "assert \"X_train\" in globals() and \"y_train\" in globals(), \"Missing X_train / y_train.\"\n",
    "assert \"X_test\"  in globals() and \"y_test\"  in globals(), \"Missing X_test / y_test.\"\n",
    "\n",
    "history_cls = class_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,           # run full 50 epochs (no early stopping here)\n",
    "    batch_size=64,\n",
    "    validation_data=(X_test, y_test),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Optional: quick test-set evaluation after training\n",
    "test_loss, test_acc = class_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"[info] Test loss: {test_loss:.4f} | Test accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBNRUAsQHiv5"
   },
   "source": [
    "Evaluate the model using the testing set. Add a comment on the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "1eTsBN5cHi9a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Test accuracy: 0.1882\n",
      "\n",
      "[info] Classification report (present classes only):\n",
      "                                      precision    recall  f1-score   support\n",
      "\n",
      "                          accounting       0.47      0.64      0.55        14\n",
      "                   airlines/aviation       0.00      0.00      0.00         3\n",
      "                           animation       0.00      0.00      0.00         1\n",
      "                   apparel & fashion       0.25      0.17      0.20        12\n",
      "             architecture & planning       0.14      0.33      0.20         6\n",
      "                     arts and crafts       0.00      0.00      0.00         1\n",
      "                          automotive       0.50      0.48      0.49        31\n",
      "                aviation & aerospace       0.00      0.00      0.00         3\n",
      "                             banking       0.00      0.00      0.00         3\n",
      "                       biotechnology       0.00      0.00      0.00         5\n",
      "                     broadcast media       0.00      0.00      0.00         3\n",
      "                  building materials       0.00      0.00      0.00         6\n",
      "     business supplies and equipment       0.00      0.00      0.00         5\n",
      "                     capital markets       0.00      0.00      0.00         0\n",
      "                           chemicals       0.00      0.00      0.00         6\n",
      "         civic & social organization       0.00      0.00      0.00         3\n",
      "                   civil engineering       0.12      0.20      0.15         5\n",
      "              commercial real estate       0.00      0.00      0.00         4\n",
      "         computer & network security       0.33      0.20      0.25         5\n",
      "                      computer games       0.00      0.00      0.00         3\n",
      "                   computer hardware       0.00      0.00      0.00         3\n",
      "                 computer networking       0.00      0.00      0.00         2\n",
      "                   computer software       0.18      0.27      0.22        51\n",
      "                        construction       0.26      0.26      0.26        39\n",
      "                consumer electronics       0.00      0.00      0.00         7\n",
      "                      consumer goods       0.17      0.07      0.10        14\n",
      "                   consumer services       0.14      0.12      0.13         8\n",
      "                           cosmetics       1.00      0.33      0.50         3\n",
      "                               dairy       0.00      0.00      0.00         1\n",
      "                     defense & space       0.00      0.00      0.00         2\n",
      "                              design       0.05      0.07      0.06        14\n",
      "                          e-learning       0.00      0.00      0.00         3\n",
      "                education management       0.09      0.06      0.07        16\n",
      " electrical/electronic manufacturing       0.18      0.18      0.18        11\n",
      "                       entertainment       0.00      0.00      0.00         6\n",
      "              environmental services       0.08      0.11      0.10         9\n",
      "                     events services       0.08      0.20      0.11        10\n",
      "                    executive office       0.00      0.00      0.00         2\n",
      "                 facilities services       0.00      0.00      0.00         6\n",
      "                             farming       0.00      0.00      0.00         3\n",
      "                  financial services       0.24      0.28      0.26        32\n",
      "                            fine art       0.00      0.00      0.00         1\n",
      "                    food & beverages       0.13      0.13      0.13        15\n",
      "                     food production       0.00      0.00      0.00         4\n",
      "                        fund-raising       0.00      0.00      0.00         1\n",
      "                           furniture       0.29      0.33      0.31         6\n",
      "                  gambling & casinos       0.00      0.00      0.00         1\n",
      "           government administration       0.00      0.00      0.00         1\n",
      "                government relations       0.00      0.00      0.00         1\n",
      "                      graphic design       0.00      0.00      0.00         4\n",
      "        health, wellness and fitness       0.15      0.09      0.11        23\n",
      "                    higher education       0.00      0.00      0.00         3\n",
      "              hospital & health care       0.00      0.00      0.00        22\n",
      "                         hospitality       0.27      0.27      0.27        15\n",
      "                     human resources       0.00      0.00      0.00         5\n",
      "                   import and export       0.00      0.00      0.00         2\n",
      "        individual & family services       0.00      0.00      0.00         5\n",
      "               industrial automation       0.00      0.00      0.00         5\n",
      "                information services       0.00      0.00      0.00         5\n",
      " information technology and services       0.32      0.31      0.31        90\n",
      "                           insurance       0.50      0.67      0.57        12\n",
      " international trade and development       0.00      0.00      0.00         2\n",
      "                            internet       0.18      0.11      0.14        37\n",
      "                  investment banking       0.00      0.00      0.00         1\n",
      "               investment management       0.00      0.00      0.00         4\n",
      "                        law practice       0.62      0.45      0.53        22\n",
      "                      legal services       0.23      0.27      0.25        11\n",
      "           leisure, travel & tourism       0.17      0.28      0.21        18\n",
      "          logistics and supply chain       0.11      0.17      0.13         6\n",
      "              luxury goods & jewelry       0.00      0.00      0.00         3\n",
      "                           machinery       0.00      0.00      0.00        15\n",
      "               management consulting       0.08      0.07      0.07        15\n",
      "                            maritime       0.00      0.00      0.00         3\n",
      "                     market research       0.00      0.00      0.00         4\n",
      "           marketing and advertising       0.35      0.32      0.34        53\n",
      "mechanical or industrial engineering       0.17      0.15      0.16        13\n",
      "                    media production       0.25      0.12      0.17         8\n",
      "                     medical devices       0.00      0.00      0.00         8\n",
      "                    medical practice       0.20      0.20      0.20        10\n",
      "                  mental health care       0.00      0.00      0.00         3\n",
      "                     mining & metals       0.00      0.00      0.00         4\n",
      "            motion pictures and film       0.00      0.00      0.00         1\n",
      "            museums and institutions       0.00      0.00      0.00         1\n",
      "                               music       0.25      0.25      0.25         4\n",
      "                      nanotechnology       0.00      0.00      0.00         1\n",
      "                          newspapers       0.00      0.00      0.00         1\n",
      "  non-profit organization management       0.00      0.00      0.00        16\n",
      "                        oil & energy       0.50      0.10      0.17        10\n",
      "                        online media       0.12      0.17      0.14         6\n",
      "              outsourcing/offshoring       0.00      0.00      0.00         3\n",
      "            packaging and containers       0.00      0.00      0.00         4\n",
      "             paper & forest products       0.00      0.00      0.00         1\n",
      "                     performing arts       0.00      0.00      0.00         2\n",
      "                     pharmaceuticals       0.00      0.00      0.00         6\n",
      "                        philanthropy       0.00      0.00      0.00         2\n",
      "                         photography       0.00      0.00      0.00         2\n",
      "                            plastics       0.00      0.00      0.00         3\n",
      "         primary/secondary education       0.00      0.00      0.00         3\n",
      "                            printing       0.14      0.25      0.18         8\n",
      "    professional training & coaching       0.00      0.00      0.00        10\n",
      "                 program development       0.00      0.00      0.00         1\n",
      " public relations and communications       0.20      0.25      0.22         4\n",
      "                       public safety       0.00      0.00      0.00         2\n",
      "                          publishing       0.00      0.00      0.00         7\n",
      "                         real estate       0.42      0.50      0.46        32\n",
      "recreational facilities and services       0.00      0.00      0.00         3\n",
      "              religious institutions       0.00      0.00      0.00         1\n",
      "            renewables & environment       0.25      0.12      0.17         8\n",
      "                            research       0.00      0.00      0.00         6\n",
      "                         restaurants       0.00      0.00      0.00         3\n",
      "                              retail       0.09      0.16      0.12        19\n",
      "         security and investigations       0.00      0.00      0.00         4\n",
      "                      semiconductors       0.00      0.00      0.00         1\n",
      "                        shipbuilding       0.00      0.00      0.00         1\n",
      "                      sporting goods       0.00      0.00      0.00         3\n",
      "                              sports       0.17      0.20      0.18         5\n",
      "             staffing and recruiting       0.40      0.35      0.38        17\n",
      "                  telecommunications       0.12      0.09      0.11        11\n",
      "                            textiles       0.00      0.00      0.00         3\n",
      "                         think tanks       0.00      0.00      0.00         1\n",
      "        translation and localization       0.00      0.00      0.00         1\n",
      "    transportation/trucking/railroad       0.00      0.00      0.00         9\n",
      "                           utilities       0.00      0.00      0.00         3\n",
      "    venture capital & private equity       0.00      0.00      0.00         2\n",
      "                          veterinary       0.00      0.00      0.00         1\n",
      "                         warehousing       0.00      0.00      0.00         1\n",
      "                           wholesale       0.08      0.12      0.10         8\n",
      "                    wine and spirits       0.00      0.00      0.00         2\n",
      "                            wireless       1.00      0.50      0.67         2\n",
      "                 writing and editing       0.00      0.00      0.00         1\n",
      "\n",
      "                            accuracy                           0.19      1068\n",
      "                           macro avg       0.09      0.08      0.08      1068\n",
      "                        weighted avg       0.19      0.19      0.18      1068\n",
      "\n",
      "[info] Confusion matrix (rows=true, cols=pred; present classes only):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accounting</th>\n",
       "      <th>airlines/aviation</th>\n",
       "      <th>animation</th>\n",
       "      <th>apparel &amp; fashion</th>\n",
       "      <th>architecture &amp; planning</th>\n",
       "      <th>arts and crafts</th>\n",
       "      <th>automotive</th>\n",
       "      <th>aviation &amp; aerospace</th>\n",
       "      <th>banking</th>\n",
       "      <th>biotechnology</th>\n",
       "      <th>...</th>\n",
       "      <th>translation and localization</th>\n",
       "      <th>transportation/trucking/railroad</th>\n",
       "      <th>utilities</th>\n",
       "      <th>venture capital &amp; private equity</th>\n",
       "      <th>veterinary</th>\n",
       "      <th>warehousing</th>\n",
       "      <th>wholesale</th>\n",
       "      <th>wine and spirits</th>\n",
       "      <th>wireless</th>\n",
       "      <th>writing and editing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accounting</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>airlines/aviation</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>animation</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>apparel &amp; fashion</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>architecture &amp; planning</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arts and crafts</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>automotive</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aviation &amp; aerospace</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>banking</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>biotechnology</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 130 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         accounting  airlines/aviation  animation  \\\n",
       "accounting                        9                  0          0   \n",
       "airlines/aviation                 0                  0          0   \n",
       "animation                         0                  0          0   \n",
       "apparel & fashion                 0                  0          0   \n",
       "architecture & planning           1                  0          0   \n",
       "arts and crafts                   0                  0          0   \n",
       "automotive                        1                  0          0   \n",
       "aviation & aerospace              0                  0          0   \n",
       "banking                           0                  0          0   \n",
       "biotechnology                     0                  0          0   \n",
       "\n",
       "                         apparel & fashion  architecture & planning  \\\n",
       "accounting                               0                        0   \n",
       "airlines/aviation                        0                        0   \n",
       "animation                                0                        0   \n",
       "apparel & fashion                        2                        0   \n",
       "architecture & planning                  0                        2   \n",
       "arts and crafts                          0                        0   \n",
       "automotive                               0                        0   \n",
       "aviation & aerospace                     0                        0   \n",
       "banking                                  0                        0   \n",
       "biotechnology                            0                        0   \n",
       "\n",
       "                         arts and crafts  automotive  aviation & aerospace  \\\n",
       "accounting                             0           0                     0   \n",
       "airlines/aviation                      1           0                     0   \n",
       "animation                              0           0                     0   \n",
       "apparel & fashion                      0           0                     0   \n",
       "architecture & planning                0           0                     0   \n",
       "arts and crafts                        0           0                     0   \n",
       "automotive                             0          15                     0   \n",
       "aviation & aerospace                   0           0                     0   \n",
       "banking                                0           0                     0   \n",
       "biotechnology                          0           0                     0   \n",
       "\n",
       "                         banking  biotechnology  ...  \\\n",
       "accounting                     0              0  ...   \n",
       "airlines/aviation              0              0  ...   \n",
       "animation                      0              0  ...   \n",
       "apparel & fashion              0              0  ...   \n",
       "architecture & planning        0              0  ...   \n",
       "arts and crafts                0              0  ...   \n",
       "automotive                     0              0  ...   \n",
       "aviation & aerospace           0              0  ...   \n",
       "banking                        0              0  ...   \n",
       "biotechnology                  0              0  ...   \n",
       "\n",
       "                         translation and localization  \\\n",
       "accounting                                          0   \n",
       "airlines/aviation                                   0   \n",
       "animation                                           0   \n",
       "apparel & fashion                                   0   \n",
       "architecture & planning                             0   \n",
       "arts and crafts                                     0   \n",
       "automotive                                          0   \n",
       "aviation & aerospace                                0   \n",
       "banking                                             0   \n",
       "biotechnology                                       0   \n",
       "\n",
       "                         transportation/trucking/railroad  utilities  \\\n",
       "accounting                                              0          0   \n",
       "airlines/aviation                                       0          0   \n",
       "animation                                               0          0   \n",
       "apparel & fashion                                       0          0   \n",
       "architecture & planning                                 0          0   \n",
       "arts and crafts                                         0          0   \n",
       "automotive                                              0          0   \n",
       "aviation & aerospace                                    0          0   \n",
       "banking                                                 0          0   \n",
       "biotechnology                                           0          0   \n",
       "\n",
       "                         venture capital & private equity  veterinary  \\\n",
       "accounting                                              0           0   \n",
       "airlines/aviation                                       0           0   \n",
       "animation                                               0           0   \n",
       "apparel & fashion                                       1           0   \n",
       "architecture & planning                                 0           0   \n",
       "arts and crafts                                         0           0   \n",
       "automotive                                              0           0   \n",
       "aviation & aerospace                                    0           0   \n",
       "banking                                                 0           0   \n",
       "biotechnology                                           0           0   \n",
       "\n",
       "                         warehousing  wholesale  wine and spirits  wireless  \\\n",
       "accounting                         0          0                 0         0   \n",
       "airlines/aviation                  0          0                 0         0   \n",
       "animation                          0          0                 0         0   \n",
       "apparel & fashion                  0          0                 0         0   \n",
       "architecture & planning            0          0                 0         0   \n",
       "arts and crafts                    0          0                 0         0   \n",
       "automotive                         0          0                 0         0   \n",
       "aviation & aerospace               0          0                 0         0   \n",
       "banking                            0          0                 0         0   \n",
       "biotechnology                      0          0                 0         0   \n",
       "\n",
       "                         writing and editing  \n",
       "accounting                                 0  \n",
       "airlines/aviation                          0  \n",
       "animation                                  0  \n",
       "apparel & fashion                          0  \n",
       "architecture & planning                    0  \n",
       "arts and crafts                            0  \n",
       "automotive                                 0  \n",
       "aviation & aerospace                       0  \n",
       "banking                                    0  \n",
       "biotechnology                              0  \n",
       "\n",
       "[10 rows x 130 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Fix: align labels with the classes actually present in y_test/pred ---\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# 1) Indices for true/pred labels\n",
    "y_test_idx = np.argmax(y_test, axis=1)\n",
    "y_pred_idx = np.argmax(class_model.predict(X_test, verbose=0), axis=1)\n",
    "\n",
    "# 2) Overall accuracy\n",
    "test_acc = accuracy_score(y_test_idx, y_pred_idx)\n",
    "print(f\"[info] Test accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# 3) Determine the exact set of labels present in this evaluation\n",
    "labels = np.sort(np.unique(np.concatenate([y_test_idx, y_pred_idx])))\n",
    "\n",
    "# 4) Build target_names aligned to labels\n",
    "if \"index_to_industry\" in globals() and isinstance(index_to_industry, dict):\n",
    "    target_names = [index_to_industry.get(i, f\"class_{i}\") for i in labels]\n",
    "elif \"industry_to_index\" in globals() and isinstance(industry_to_index, dict):\n",
    "    inv = {v: k for k, v in industry_to_index.items()}\n",
    "    target_names = [inv.get(i, f\"class_{i}\") for i in labels]\n",
    "else:\n",
    "    # Fallback: just use the integer IDs as names\n",
    "    target_names = [f\"class_{i}\" for i in labels]\n",
    "\n",
    "# 5) Classification report restricted to the present labels\n",
    "print(\"\\n[info] Classification report (present classes only):\")\n",
    "print(classification_report(\n",
    "    y_test_idx, y_pred_idx,\n",
    "    labels=labels,\n",
    "    target_names=target_names,\n",
    "    zero_division=0\n",
    "))\n",
    "\n",
    "# 6) Confusion matrix restricted to the same label set\n",
    "cm = confusion_matrix(y_test_idx, y_pred_idx, labels=labels)\n",
    "cm_df = pd.DataFrame(cm, index=target_names, columns=target_names)\n",
    "print(\"[info] Confusion matrix (rows=true, cols=pred; present classes only):\")\n",
    "display(cm_df.head(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sHIdfP4tiYnt"
   },
   "source": [
    "We will now define a function called `classify_slogan` which takes a slogan as input and predicts the industry it belongs to using the trained model, `class_model`.  \n",
    "\n",
    "Carefully follow the code below and complete the missing parts (indicated by ellipses) as guided by the comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "fNEmTt6piYvE"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'marketing and advertising'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Predict the industry for a single slogan using the trained classifier \n",
    "\n",
    "\n",
    "def classify_slogan(slogan: str) -> str:\n",
    "    #  Clean the input text using the same preprocessing as training\n",
    "    slogan = preprocess_text(slogan)\n",
    "\n",
    "    #  Convert cleaned text → sequence of token IDs\n",
    "    sequence = tokenizer.texts_to_sequences([slogan])  # list of one list\n",
    "\n",
    "    #  Pad to the classifier's expected input length\n",
    "    padded_sequence = pad_sequences(\n",
    "        sequence,\n",
    "        maxlen=max_seq_len,   # full-slogan length used for classifier\n",
    "        padding=\"pre\",\n",
    "        truncating=\"pre\",\n",
    "        value=0\n",
    "    )\n",
    "\n",
    "    #  Predict class probabilities and take the argmax\n",
    "    prediction = class_model.predict(padded_sequence, verbose=0)   # shape: (1, num_classes)\n",
    "    predicted_index = int(np.argmax(prediction, axis=1)[0])\n",
    "\n",
    "    #  Return the industry name for that index\n",
    "    return industries[predicted_index]\n",
    "\n",
    "# Example:\n",
    "classify_slogan(\"affordable coverage for your family\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RZbZQ-TNoDRj"
   },
   "source": [
    "## Combining the two models\n",
    "\n",
    "Run the code cell below to combine the two models: we will first generate a slogan for a company in the \"internet\" industry, then pass the generated slogan to the slogan classifier to see if it correctly classifies it as internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "SMpjs0rFoILz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Slogan: internet web design agency seo digital marketing agency in dubai mena industries pune pune ga or immigration and prime supplier pune\n",
      "Predicted Industry: marketing and advertising\n"
     ]
    }
   ],
   "source": [
    "industry = \"internet\"\n",
    "generated_slogan = generate_slogan(industry)\n",
    "predicted_industry = classify_slogan(generated_slogan)\n",
    "\n",
    "print(f\"Generated Slogan: {generated_slogan}\")\n",
    "print(f\"Predicted Industry: {predicted_industry}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_niR0aS5zlQC"
   },
   "source": [
    "Compare the results and comment on any differences you notice between the generated slogans and the classifier’s predictions in the markdown cell below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R17KFEDLzlda"
   },
   "source": [
    "## Generated vs. Classified Slogans — Brief Comparison\n",
    "**What I looked at**\n",
    "\n",
    "- I generated several slogans per industry using the next-word LSTM (seeded with an industry), then fed those same lines into the classifier to see whether it predicts the intended industry.\n",
    "**Where they agree (good cases)**\n",
    "- When the generated text includes industry-specific tokens (e.g., “coverage, policy, premium” for insurance), the classifier usually -returns the same industry.\n",
    "- Longer, more specific generations tend to be classified correctly more often than very short ones.\n",
    "**Where they diverge (common mismatches)**\n",
    "- Generic phrasing (“quality service”, “trusted solutions”, “we care”) lacks clear signals, so the classifier may pick a high-frequency class rather than the intended one.\n",
    "- Short outputs (few tokens) carry little context; the classifier can swing to a confusable industry.\n",
    "- Imbalanced data: industries with more training examples dominate both the generator’s vocabulary and the classifier’s decision boundary, increasing off-class predictions for rarer labels.\n",
    "- Vocabulary limits: uncommon or out-of-vocab terms get mapped to 0 or ignored, weakening cues the classifier needs.\n",
    "- Training objective gap: the generator is trained for next-word prediction, not industry accuracy; the classifier is trained for industry discrimination on full sequences. These goals aren’t identical.\n",
    "  \n",
    "**Takeaways**\n",
    "- Agreement is highest when the generation clearly names the domain and uses distinctive terms.\n",
    "- Disagreements are mostly tied to generic language, class imbalance, and the short length of slogans.\n",
    "- For stronger alignment, consider (if allowed in your template):\n",
    "    - nudging generation to include industry keywords (longer seed, slightly higher max length),\n",
    "    - modestly increasing epochs or adjusting temperature to improve diversity without losing relevance,\n",
    "    - and, for the classifier, using class weighting or collecting a few more examples per rare industry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
